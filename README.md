# Hugging Face Model Inference API (Docker + Flask + Gunicorn)

This project demonstrates deploying a pretrained Hugging Face model as a containerized inference API.

The service exposes a simple `/predict` endpoint that accepts text input and returns a sentiment classification generated by a lightweight DistilBERT model.

The goal of this demo is to showcase:
- wrapping a pretrained model behind a web API
- handling inference inside a containerized environment
- supporting multiple incoming requests using Gunicorn workers
- providing a simple notebook client to exercise the API

## Running the Service (Docker)

### Build the image

```bash
docker build -t ml-inference-api .
```

### Run the container
```bash
docker run -p 8000:8000 ml-inference-api
```

